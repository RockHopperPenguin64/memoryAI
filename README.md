# memoryAI - Long-Term Memory Enhanced GPT Model
â†’[License&Policy](https://github.com/RockHopperPenguin64/memoryAI/blob/main/License.md)

â†’[ğŸ‡¯ğŸ‡µæ—¥æœ¬èªã¯ã“ã¡ã‚‰](https://github.com/RockHopperPenguin64/memoryAI/blob/main/jpREADME.md)

## Overview

**memoryAI** is a custom-built Transformer-based GPT model designed to support long-term memory capabilities.  
The project aims to enable contextual dialogue by integrating a memory retrieval system that references past conversations stored in a SQL database.

 

## Key Features

- **Custom GPT Architecture**  
  Built from scratch using PyTorch, including RMSNorm, MultiHeadAttention, Positional Embedding, and configurable depth and width.

- **Japanese Language Support**  
  Tokenization is handled via SentencePiece and Juman++, optimized for Japanese text processing.

- **Sampling and Inference Control**  
  Implements top-k and top-p filtering, temperature scaling, and repetition penalty for controlled text generation.

- **Memory Retrieval Logic (Planned)**  
  A SQL-based memory system will allow the model to retrieve relevant past dialogue based on token-level matching and weighted scoring.  
  Top-k filtering and re-ranking will be used to select the most contextually appropriate memory.

- **Training Pipeline**  
  Includes mixed precision training (GradScaler, autocast), cosine learning rate scheduling, and perplexity-based evaluation.

## Development Roadmap

- **Phase 1: Model Core Completion**  âœ…[Completed]
  - Finalize the GPT architecture and inference logic.
  - Implement a basic training pipeline.

- **Phase 2: Training & Optimization** â³[In Progress]
  - Begin model training with Japanese datasets.
  - Optimize the training process using mixed precision and a cosine learning rate scheduler.

- **Phase 3: Memory System Integration** ğŸ”œ[Planned]
  - Design and implement the SQL-based memory retrieval logic.
  - Integrate the memory system to allow the model to reference past conversations.

- **Phase 4: Contextual Dialogue** ğŸš€[Future]
  - Finalize the overall system to enable long-term memory-aware responses.
 

## Project Structure
model/

â””â”€â”€ **gpt_model.py**    _# Transformer model implementation_

config/

â””â”€â”€ **gpt_config.py**   _# Model configuration via dataclass_

infrence/

â”œâ”€â”€ **generate.py**     _# Text generation logic_

â””â”€â”€ **sampling.py**     _# Sampling and filtering functions_

train/

â”œâ”€â”€ **train.py**       _ # Training loop with optimizer and scheduler _

â”œâ”€â”€ **evaluate.py**     _# Evaluation metrics (e.g., perplexity) _

â””â”€â”€ **dataset.py**      _# Dataset loader and preprocessing_

utils/ 

â””â”€â”€ **lr_scheduler.py** _# Cosine learning rate scheduler_

 

## Technologies Used

- Python / PyTorch  
- SentencePiece / Juman++  
- SQL (planned for memory storage and retrieval)  
- Mixed Precision Training  
- Cosine Learning Rate Scheduler

